# FlashAttention

## 0x00 说明

包含以下内容：

- [X] flash_attn_1_fwd_f32_kernel 
- [ ] flash_attn_2_fwd_f32_kernel
- [ ] flash_attn_2_fwd_f16_kernel
- [x] flash_attn_2_fwd_f16_mma_m16n8k16_kernel
- [X] PyTorch bindings

### 运行测试   
```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada 
python3 flash_attn.py
```
日志如下：
```bash
--------------------------------------------------------------------------------
       out_fa1f32: [-0.07522935, -0.06757538, -0.30396557], time:0.77749610ms
   out_fa1f32(v2): [-0.07522935, -0.06757538, -0.30396557], time:0.77480674ms
   out_attnf32_th: [-0.07522935, -0.06757541, -0.30396566], time:0.05429983ms
--------------------------------------------------------------------------------
    out_fa2mmaf16: [-0.07525635, -0.06762695, -0.30395508], time:0.01422763ms
out_fa2mmaf16(v2): [-0.07525635, -0.06762695, -0.30395508], time:0.01072645ms
   out_attnf16_th: [-0.07525635, -0.06756592, -0.30395508], time:0.05322218ms
--------------------------------------------------------------------------------
```
