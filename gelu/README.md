# GELU

## 0x00 说明

包含以下内容：

- [X] gelu_f32_kernel
- [X] gelu_f32x4_kernel(float4向量化版本)
- [X] gelu_f16_kernel
- [X] gelu_f16x2_kernel(half2向量化)
- [X] gelu_f16x8_kernel(unpack版本)
- [X] gelu_f16x8_pack_kernel(pack版本)
- [X] PyTorch bindings


## 测试

对于半精度(half)的GELU操作，由于CUDA的半精度计算中并不包含tanh操作，因此需要使用hexp来替代对应的操作，因此会引入较大的误差。（或许可以考虑从汇编上解决这个问题）;而torch是通过转化数据类型完成的。想要测试很简单，修改一下cu中f16里面的代码做一下强制类型转换即可：

```cpp
// line 96
y[idx] = HALF_GELU_OPS(__half2float(v));
// line 109 , line 110
reg_y.x = HALF_GELU_OPS(__half2float(reg_x.x)); 
reg_y.y = HALF_GELU_OPS(__half2float(reg_x.y)); 
```
测试结果如下（由于不是所有数据都会掉误差所以取了会有误差的情况，可见修改后out_f16和out_f16x2的结果和torch相同了）：
```bash
                                        S=2048, K=4096
           out_f32: [-0.08196318, -0.1613517], time:0.13425708ms
         out_f32x4: [-0.08196318, -0.1613517], time:0.14128804ms
        out_f32_th: [-0.08196313, -0.1613517], time:0.08195782ms
-------------------------------------------------------------------------------------
           out_f16: [-0.08197021, -0.16137695], time:0.12120271ms
         out_f16x2: [-0.08197021, -0.16137695], time:0.12122369ms
         out_f16x8: [-0.08251953, -0.16137695], time:0.04196978ms
     out_f16x8pack: [-0.08251953, -0.16137695], time:0.04215288ms
        out_f16_th: [-0.08197021, -0.16137695], time:0.04287958ms

```
相关参考：
- (pytorch-c10-BFloat16.h)[https://github.com/pytorch/pytorch/blob/main/c10/util/BFloat16.h]
- (math ptx)[https://github.com/pavanky/math_ptx]

此外仿照torch实现了在float下tanh和none两种近似下的GELU函数，可以在gelu.cu的宏中进行修改实现不同的版本的编译。

```bash
# 只测试Ada架构 不指定默认编译所有架构 耗时较长: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada 
python3 gelu.py
```

输出（不做类型转换导致half误差）:

```bash
-------------------------------------------------------------------------------------
                                        S=1024, K=1024
           out_f32: [0.93880296, 0.15988638], time:0.02785468ms
         out_f32x4: [0.93880296, 0.15988638], time:0.02076554ms
        out_f32_th: [0.93880296, 0.15988638], time:0.01221609ms
-------------------------------------------------------------------------------------
           out_f16: [0.93798828, 0.15979004], time:0.00964093ms
         out_f16x2: [0.93798828, 0.15979004], time:0.00525022ms
         out_f16x8: [0.93798828, 0.15979004], time:0.00469351ms
     out_f16x8pack: [0.93798828, 0.15979004], time:0.00465655ms
        out_f16_th: [0.93847656, 0.15991211], time:0.00669861ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=2048
           out_f32: [-0.14857908, 0.10128548], time:0.03697181ms
         out_f32x4: [-0.14857908, 0.10128548], time:0.03849959ms
        out_f32_th: [-0.14857908, 0.10128548], time:0.02257371ms
-------------------------------------------------------------------------------------
           out_f16: [-0.14904785, 0.10119629], time:0.01546693ms
         out_f16x2: [-0.14904785, 0.10119629], time:0.01501513ms
         out_f16x8: [-0.14904785, 0.10119629], time:0.01015544ms
     out_f16x8pack: [-0.14904785, 0.10119629], time:0.01015282ms
        out_f16_th: [-0.14855957, 0.10125732], time:0.01221085ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=1024, K=4096
           out_f32: [-0.16260667, 2.28252459], time:0.07104182ms
         out_f32x4: [-0.16260667, 2.28252459], time:0.08304977ms
        out_f32_th: [-0.16260667, 2.28252459], time:0.04243922ms
-------------------------------------------------------------------------------------
           out_f16: [-0.16296387, 2.28125], time:0.02782536ms
         out_f16x2: [-0.16296387, 2.28125], time:0.02191663ms
         out_f16x8: [-0.16296387, 2.28125], time:0.02220559ms
     out_f16x8pack: [-0.16296387, 2.28125], time:0.02232957ms
        out_f16_th: [-0.16259766, 2.28320312], time:0.02265978ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=1024
           out_f32: [-0.16840045, -0.14960197], time:0.05070662ms
         out_f32x4: [-0.16840045, -0.14960197], time:0.03644156ms
        out_f32_th: [-0.16840045, -0.14960195], time:0.02212596ms
-------------------------------------------------------------------------------------
           out_f16: [-0.16845703, -0.1496582], time:0.02071333ms
         out_f16x2: [-0.16845703, -0.1496582], time:0.01206446ms
         out_f16x8: [-0.16845703, -0.1496582], time:0.00981784ms
     out_f16x8pack: [-0.16845703, -0.1496582], time:0.00988960ms
        out_f16_th: [-0.16845703, -0.1496582], time:0.01215363ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=2048
           out_f32: [-0.16697021, -0.16277096], time:0.06218576ms
         out_f32x4: [-0.16697021, -0.16277096], time:0.06344438ms
        out_f32_th: [-0.16697019, -0.16277094], time:0.04222322ms
-------------------------------------------------------------------------------------
           out_f16: [-0.16699219, -0.16271973], time:0.02624702ms
         out_f16x2: [-0.16699219, -0.16271973], time:0.02568126ms
         out_f16x8: [-0.16699219, -0.16271973], time:0.02205300ms
     out_f16x8pack: [-0.16699219, -0.16271973], time:0.02210712ms
        out_f16_th: [-0.16699219, -0.16271973], time:0.02253604ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=2048, K=4096
           out_f32: [-0.09021921, -0.16487332], time:0.13927341ms
         out_f32x4: [-0.09021921, -0.16487332], time:0.14096951ms
        out_f32_th: [-0.09021921, -0.16487332], time:0.08194113ms
-------------------------------------------------------------------------------------
           out_f16: [-0.09033203, -0.16503906], time:0.05144143ms
         out_f16x2: [-0.09033203, -0.16503906], time:0.04174685ms
         out_f16x8: [-0.09033203, -0.16503906], time:0.04198074ms
     out_f16x8pack: [-0.09033203, -0.16503906], time:0.04212999ms
        out_f16_th: [-0.09020996, -0.16491699], time:0.04287744ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=1024
           out_f32: [0.07282269, -0.06332674], time:0.09058189ms
         out_f32x4: [0.07282269, -0.06332674], time:0.06340218ms
        out_f32_th: [0.07282269, -0.06332674], time:0.04206586ms
-------------------------------------------------------------------------------------
           out_f16: [0.07281494, -0.06335449], time:0.03970504ms
         out_f16x2: [0.07281494, -0.06335449], time:0.02199268ms
         out_f16x8: [0.07281494, -0.06335449], time:0.02213860ms
     out_f16x8pack: [0.07281494, -0.06335449], time:0.02209067ms
        out_f16_th: [0.07281494, -0.06335449], time:0.02286220ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=2048
           out_f32: [-0.11318169, -0.10836542], time:0.12297416ms
         out_f32x4: [-0.11318169, -0.10836542], time:0.12383652ms
        out_f32_th: [-0.11318169, -0.10836542], time:0.08190846ms
-------------------------------------------------------------------------------------
           out_f16: [-0.11315918, -0.10888672], time:0.05153990ms
         out_f16x2: [-0.11315918, -0.10888672], time:0.04872131ms
         out_f16x8: [-0.11315918, -0.10888672], time:0.04182482ms
     out_f16x8pack: [-0.11315918, -0.10888672], time:0.04196978ms
        out_f16_th: [-0.11322021, -0.1083374], time:0.04286408ms
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------
                                        S=4096, K=4096
           out_f32: [-0.16762884, 0.33026037], time:0.26759410ms
         out_f32x4: [-0.16762884, 0.33026037], time:0.27700567ms
        out_f32_th: [-0.16762884, 0.33026037], time:0.16148257ms
-------------------------------------------------------------------------------------
           out_f16: [-0.16760254, 0.33032227], time:0.10299659ms
         out_f16x2: [-0.16760254, 0.33032227], time:0.08103538ms
         out_f16x8: [-0.16760254, 0.33032227], time:0.08191633ms
     out_f16x8pack: [-0.16760254, 0.33032227], time:0.08227539ms
        out_f16_th: [-0.16760254, 0.33032227], time:0.08262110ms
-------------------------------------------------------------------------------------
```
